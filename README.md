# llm-course

An AI LLM prototype written in plain python, only for personnal educational purpose. Work in progress.

We build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3.

This course was possible by the help of those two videos:

- [Andrej Karpathy - Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)

- [freeCodeCamp.org - Create a Large Language Model from Scratch with Python â€“ Tutorial](https://www.youtube.com/watch?v=UU1WVnMk4E8)

It was made for educationnal purpose and understanding what is happening under the hood.

Using these tools : 

- Python Anaconda, langage distribution made for research and deep-learning

- Ipykernel, for handling python3 kernels with virtual environments

- Pytorch, an optimized tensor library for deep learning

- Cuda, for parallel GPU computing

- Jupyter for documentational programming

## Setup

```bash
bash setup.sh
```

## Launch 

```bash
bash start.sh
```
