# llm from scratch

An AI LLM prototype written in plain python, only for personnal educational purpose. Work in progress.

It is a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3.

It was made for educationnal purpose only and understanding what is happening under the hood.

<strong> No AI were used in any of the project nor code. </strong>

I tried to learn and resume a course based on those two great videos :

- [Andrej Karpathy - Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)

- [freeCodeCamp.org - Create a Large Language Model from Scratch with Python â€“ Tutorial](https://www.youtube.com/watch?v=UU1WVnMk4E8)

Using these tools : 

- Python Anaconda, langage distribution made for research and deep-learning

- Ipykernel, for handling python3 kernels with virtual environments

- Pytorch, an optimized tensor library for deep learning

- CUDA, for parallel GPU computing

- Jupyter, for python notebooks

## Setup

```bash
bash setup.sh
```

## Launch 

```bash
bash run.sh
```
