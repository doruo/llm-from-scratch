{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5bc47e2",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f4fcbc",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "First, we read a book file to create our vocabulary. It will then be used to train the model.\n",
    "\n",
    "We create with this a tokenizer that is divided into two steps: \n",
    "\n",
    "- encode text into integers sorted set, \n",
    "\n",
    "- and decode integers input into original text.\n",
    "\n",
    "This tokenizer works with char-level tokenizing, it means that in each prompt, it will encode each character. It is not the most efficient, but we are gonna stay on char-level to simplify the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"'()*+,-./0123456789:;<>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz£﻿\n"
     ]
    }
   ],
   "source": [
    "# Rename it if you want to try on entire book content\n",
    "file_name = \"./data/journey_to_the_center_of_the_earth.txt\"\n",
    "\n",
    "# Fetch book content\n",
    "fd = open(file_name, encoding=\"utf8\")\n",
    "file_content = fd.read()\n",
    "\n",
    "vocab = sorted(set(file_content))\n",
    "\n",
    "def string_to_int(): \n",
    "   return { char:i for i, char in enumerate(vocab) }\n",
    "\n",
    "def int_to_string():\n",
    "   return { i:char for i, char in enumerate(vocab) }\n",
    "\n",
    "def encode(chars):\n",
    "   return [ string_to_int()[c] for c in chars ]\n",
    "\n",
    "def decode(integers):\n",
    "   return ''.join([ int_to_string()[i] for i in integers ])\n",
    "\n",
    "print(''.join(vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7028702",
   "metadata": {},
   "source": [
    "Here is an application example of the tokenizer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86a1590b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data: [83, 30, 35, 28, 43, 47, 32, 45, 1, 14, 0, 0, 40, 52, 1, 48, 41, 30, 39, 32, 1, 40, 28, 38, 32, 46, 1, 28, 1, 34, 45, 32, 28, 47, 1, 31, 36, 46, 30, 42, 49, 32, 45, 52, 0, 0, 0, 39, 70, 70, 66, 64, 69, 62, 1, 57, 56, 58, 66, 1, 75, 70, 1, 56, 67, 67, 1, 75, 63, 56]\n",
      "Decoded data: ﻿CHAPTER 1\n",
      "\n",
      "MY UNCLE MAKES A GREAT DISCOVERY\n",
      "\n",
      "\n",
      "Looking back to all tha\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = encode(file_content)[:70]\n",
    "\n",
    "print (\"Encoded data:\", data)\n",
    "print (\"Decoded data:\", decode(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffe3b2",
   "metadata": {},
   "source": [
    "# Training and validation\n",
    "\n",
    "We splits data into prediction and validation, because we do not want to make the model to copy exactly the content of data, but try to product content similar to the data, not the exact one. \n",
    "\n",
    "So we never feed the entire data into the transform, but more of chunks of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2ac67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83, 30, 35, 28, 43, 47, 32, 45]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent = 90 # Cut data into 90% / 10%\n",
    "n = int(len(data) * (percent/100))\n",
    " \n",
    "train_data = data[:n]\n",
    "validation_data = data[n:]\n",
    "\n",
    "block_size = 8\n",
    "train_data[:block_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ee875",
   "metadata": {},
   "source": [
    "## Context and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9884ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~\n",
      "x chunk: ﻿CHAPTER 1\n",
      "\n",
      "MY UNCLE\n",
      "y chunk CHAPTER 1\n",
      "\n",
      "MY UNCLE \n",
      "~~~~~~~~~~~~~~~~~~~\n",
      "-------\n",
      "Step 0\n",
      "[﻿], [C]\n",
      "-------\n",
      "Step 1\n",
      "[﻿C], [H]\n",
      "-------\n",
      "Step 2\n",
      "[﻿CH], [A]\n",
      "-------\n",
      "Step 3\n",
      "[﻿CHA], [P]\n",
      "-------\n",
      "Step 4\n",
      "[﻿CHAP], [T]\n",
      "-------\n",
      "Step 5\n",
      "[﻿CHAPT], [E]\n",
      "-------\n",
      "Step 6\n",
      "[﻿CHAPTE], [R]\n",
      "-------\n",
      "Step 7\n",
      "[﻿CHAPTER], [ ]\n",
      "-------\n",
      "Step 8\n",
      "[﻿CHAPTER ], [1]\n",
      "-------\n",
      "Step 9\n",
      "[﻿CHAPTER 1], [\n",
      "]\n",
      "-------\n",
      "Step 10\n",
      "[﻿CHAPTER 1\n",
      "], [\n",
      "]\n",
      "-------\n",
      "Step 11\n",
      "[﻿CHAPTER 1\n",
      "\n",
      "], [M]\n",
      "-------\n",
      "Step 12\n",
      "[﻿CHAPTER 1\n",
      "\n",
      "M], [Y]\n",
      "-------\n",
      "Step 13\n",
      "[﻿CHAPTER 1\n",
      "\n",
      "MY], [ ]\n",
      "-------\n",
      "Step 14\n",
      "[﻿CHAPTER 1\n",
      "\n",
      "MY ], [U]\n",
      "-------\n",
      "Step 15\n",
      "[﻿CHAPTER 1\n",
      "\n",
      "MY U], [N]\n",
      "-------\n",
      "Step 16\n",
      "[﻿CHAPTER 1\n",
      "\n",
      "MY UN], [C]\n",
      "-------\n",
      "Step 17\n",
      "[﻿CHAPTER 1\n",
      "\n",
      "MY UNC], [L]\n",
      "-------\n",
      "Step 18\n",
      "[﻿CHAPTER 1\n",
      "\n",
      "MY UNCL], [E]\n",
      "-------\n",
      "Step 19\n",
      "[﻿CHAPTER 1\n",
      "\n",
      "MY UNCLE], [ ]\n"
     ]
    }
   ],
   "source": [
    "percent = 90 # Cut data into 90% / 10%\n",
    "n = int(len(file_content) * (percent/100))\n",
    " \n",
    "train_data = file_content[:n+1]\n",
    "validation_data = file_content[n:]\n",
    "\n",
    "block_size = 20\n",
    "\n",
    "xChunk = train_data[:block_size]\n",
    "yChunk = train_data[1:block_size+1]\n",
    "\n",
    "print (\"~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"x chunk:\",xChunk)\n",
    "print(\"y chunk\", yChunk)\n",
    "print (\"~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "context = \"\"\n",
    "target = \"\"\n",
    "for i in range(block_size):\n",
    "    context = xChunk[:i+1]\n",
    "    target = yChunk[i]\n",
    "\n",
    "    print(\"-------\")\n",
    "    print(\"Step\", i)\n",
    "    print(\"[\"+context+\"], [\"+target+\"]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
