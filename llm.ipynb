{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5bc47e2",
   "metadata": {},
   "source": [
    "# LLM Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427b63f",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a8914",
   "metadata": {},
   "source": [
    "We build a Generatively Pretrained Transformer (GPT), following the paper \"Attention is All You Need\" and OpenAI's GPT-2 / GPT-3.\n",
    "\n",
    "The goal is to understand what is happening under the hoodn, by taking baby steps and make it accessible for everyone.\n",
    "\n",
    "Made for educationnal purpose only. \n",
    "\n",
    "This course was possible with the help of those two videos :\n",
    "\n",
    "- [Andrej Karpathy - Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "\n",
    "- [freeCodeCamp.org - Create a Large Language Model from Scratch with Python â€“ Tutorial](https://www.youtube.com/watch?v=UU1WVnMk4E8)\n",
    "\n",
    "\n",
    "We are gonna use these tools to help us : \n",
    "\n",
    "- Python Anaconda, langage distribution made for research and deep-learning\n",
    "\n",
    "- Ipykernel, for handling python3 kernels with virtual environments\n",
    "\n",
    "- Pytorch, an optimized tensor library for deep learning\n",
    "\n",
    "- Cuda, for parallel GPU computing\n",
    "\n",
    "- Jupyter for documentational programming\n",
    "\n",
    "Of course, no AI were used for code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f4fcbc",
   "metadata": {},
   "source": [
    "## Character-level Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b76b2",
   "metadata": {},
   "source": [
    "### Text data fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a6cd8",
   "metadata": {},
   "source": [
    "First, we read a text to create our vocabulary.\n",
    "\n",
    "For this course, I choose the 1864 novel \"Journey to the Center of the Earth\" of french writer Jules Verne.\n",
    "\n",
    "Taken from free-ebooks [Project Gutemberg website](https://www.gutenberg.org/ebooks/18857)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53bf3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./data/journey_to_the_center_of_the_earth.txt\"\n",
    "\n",
    "# Fetch book content\n",
    "file_descriptor = open(file_name, encoding=\"utf8\")\n",
    "text = file_descriptor.read()\n",
    "\n",
    "print(text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69db3843",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f35461",
   "metadata": {},
   "source": [
    "A vocabulary is a sorted set of every character that appears in the text.\n",
    "\n",
    "It will then be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b150f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f5e4b3",
   "metadata": {},
   "source": [
    "### Encoding and decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cbc29b",
   "metadata": {},
   "source": [
    "With this vocabulary, we can now create a tokenizer, that consists of two parts : \n",
    "\n",
    "- encode text into integers sorted set, \n",
    "\n",
    "- and decode integers input into original text.\n",
    "\n",
    "This tokenizer works with char-level tokenizing, it means that in each prompt, it will encode each character. \n",
    "\n",
    "Each character will be affeted by an integer.\n",
    "\n",
    "It is not the most efficient, but we are gonna stay on char-level to simplify the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c1f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_int(): \n",
    "   return { char:i for i, char in enumerate(vocab) }\n",
    "\n",
    "def int_to_string():\n",
    "   return { i:char for i, char in enumerate(vocab) }\n",
    "\n",
    "print(string_to_int())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e1a61",
   "metadata": {},
   "source": [
    "Then we can now use our converters to apply encoding and decoding :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(chars):\n",
    "   return [ string_to_int()[c] for c in chars ]\n",
    "\n",
    "def decode(integers):\n",
    "   return ''.join([ int_to_string()[i] for i in integers ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7028702",
   "metadata": {},
   "source": [
    "Here is an application example of the tokenizer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = text[:8]\n",
    "\n",
    "print(\"Original data:\", data)\n",
    "print (\"Encoded data:\", encode(data))\n",
    "print (\"Decoded data:\", decode(encode(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffe3b2",
   "metadata": {},
   "source": [
    "## Predictions from contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d24eb",
   "metadata": {},
   "source": [
    "### Training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03dbf5",
   "metadata": {},
   "source": [
    "We splits data into prediction and validation, because we do not want to make the model to copy exactly the content of data, but try to product content similar to the data, not the exact one. \n",
    "\n",
    "So we never feed the entire data into the transform, but blocks of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ac67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 0.9 # Splits data\n",
    "n = int(len(text) * percent) # Calculated split\n",
    " \n",
    "train_data = text[:n+1] # first 90%\n",
    "validation_data = text[n:] # remaining 10%\n",
    "\n",
    "print(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b6752",
   "metadata": {},
   "source": [
    "### Contexts and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddefaf2",
   "metadata": {},
   "source": [
    "The main goal of a model is from given context, achieve to predict what comes next. \n",
    "\n",
    "For example, if our text data is \"Hello\" :\n",
    "\n",
    "- If context: \"H\" -> target would predict: \"E\",\n",
    "\n",
    "- Then if context: \"E\" -> target would predict: \"L\",\n",
    "\n",
    "- Then if context: \"L\" -> target would predict: \"L\",\n",
    "\n",
    "- Then if context: \"L\" -> target would predict: \"O\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34560c8",
   "metadata": {},
   "source": [
    "### Usage of character blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d6e34",
   "metadata": {},
   "source": [
    "It would be better to work with blocks of characters, to gains more accuraty, and generate texts from given one. \n",
    "\n",
    "For example, if we keep our text data as \"Hello\" :\n",
    "\n",
    "- If context: \"H\" -> target would predict: \"E\",\n",
    "\n",
    "- Then if context: \"HE\" -> target would predict: \"L\",\n",
    "\n",
    "- Then if context: \"HEL\" -> target would predict: \"L\",\n",
    "\n",
    "- Then if context: \"HELL\" -> target would predict: \"O\".\n",
    "\n",
    "As you can see, a target always consists of the next predicted character, that comes after the context.\n",
    "\n",
    "So we can agree that :\n",
    "\n",
    "```python\n",
    "context_block = data[:block_size]\n",
    "\n",
    "target_block = data[1:block_size+1]\n",
    "```\n",
    "\n",
    "Here is an application, with a block size of 7 characters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9884ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 7\n",
    "\n",
    "context_block = train_data[:block_size]\n",
    "\n",
    "target_block = train_data[1:block_size+1]\n",
    "\n",
    "print(\"Context block :\",context_block)\n",
    "print(\"Target block :\", target_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4de8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(block_size):\n",
    "\n",
    "    context = context_block[:i+1]\n",
    "    \n",
    "    target = target_block[i]\n",
    "\n",
    "    print(\"-------\")\n",
    "    print(\"Context: [\"+context+\"], target: [\"+target+\"]\")\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e012243",
   "metadata": {},
   "source": [
    "### Batchs for parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f64a90f",
   "metadata": {},
   "source": [
    "\n",
    "In a block, it would be more efficient to analyse predictions in parrallel within the block, because they are independant.\n",
    "\n",
    "So each step from previous loop, is a element of the batch.\n",
    "\n",
    "If batch_size = 4, a context batch would contains:\n",
    "\n",
    "- [[],[Lo],[Loo],[Loop]]\n",
    "\n",
    "We implement batch creation and processing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35136d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Generate a small batch of data of contexts x and targets y\n",
    "# Two possible split: training or validation\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"training\" else validation_data\n",
    "    indexes_blocks = torch.ranint(len(data) - block_size, (block_size,))\n",
    "\n",
    "    batch_contexts = torch.stack([data[i:i+block_size] for i in indexes_blocks])\n",
    "    batch_targets = torch.stack([data[i+1:i+block_size+1] for i in indexes_blocks])\n",
    "\n",
    "    return batch_contexts, batch_targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0432a",
   "metadata": {},
   "source": [
    "To do parrallel computing, the best option is to use GPU computing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
