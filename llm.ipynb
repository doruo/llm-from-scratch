{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5bc47e2",
   "metadata": {},
   "source": [
    "# LLM From Stratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427b63f",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a8914",
   "metadata": {},
   "source": [
    "An AI LLM prototype written in plain python, only for personnal educational purpose. WORK IN PROGRESS.\n",
    "\n",
    "It is a Generatively Pretrained Transformer (GPT), following the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) and OpenAI's [GPT-2](https://huggingface.co/openai-community/gpt2) / [GPT-3](https://fr.wikipedia.org/wiki/GPT-3).\n",
    "\n",
    "It was made for educationnal purpose only and understanding what is happening under the hood.\n",
    "\n",
    "<strong> No AI were used in any of the project nor code. </strong>\n",
    "\n",
    "I tried to learn and resume a course based on those two great videos :\n",
    "\n",
    "- [Andrej Karpathy - Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "\n",
    "- [freeCodeCamp.org - Create a Large Language Model from Scratch with Python – Tutorial](https://www.youtube.com/watch?v=UU1WVnMk4E8)\n",
    "\n",
    "Using these tools : \n",
    "\n",
    "- [Anaconda](https://www.anaconda.com/docs), langage distribution made for research and deep-learning\n",
    "\n",
    "- [PyTorch](https://pytorch.org/), an optimized tensor library for deep learning\n",
    "\n",
    "- [Nvidia CUDA API](https://www.anaconda.com/docs/getting-started/working-with-conda/packages/gpu-packages), for [GPU computation](https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units)\n",
    "\n",
    "- [Jupyter Notebook](https://jupyter.org/), for creating and sharing computational documents ([web sample here](https://jupyter.org/try-jupyter/notebooks/?path=notebooks/Intro.ipynb)).\n",
    "\n",
    "- [Ipykernel](https://pypi.org/project/ipykernel/), IPython Kernel for Jupyter, we'll use it for creating virtual environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40b19cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f4fcbc",
   "metadata": {},
   "source": [
    "## Character-level Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b76b2",
   "metadata": {},
   "source": [
    "### Text data fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a6cd8",
   "metadata": {},
   "source": [
    "First, we need to read a text to create our vocabulary.\n",
    "\n",
    "For this course, I choose the 1864 novel \"Journey to the Center of the Earth\", written by Jules Verne.\n",
    "\n",
    "Taken from free-ebooks [Project Gutemberg website](https://www.gutenberg.org/ebooks/18857) where you can find more if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53bf3f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking back to all that has occurred to me since that eventful day, I\n",
      "am scarcely able to believe in the reality of my adventures. They were\n",
      "truly so wonderful that even now I am bewildered when I think of them.\n",
      "\n",
      "My uncle was a German, having married my mother's sister, an\n",
      "Englishwoman. Being very much attached to his fatherless nephew, he\n",
      "invited me to study under him in his home in the fatherl\n"
     ]
    }
   ],
   "source": [
    "# We can also use data/chapter1.txt for a more lightweight input\n",
    "file_name = \"data/journey_to_the_center_of_the_earth.txt\" \n",
    "\n",
    "# Fetch book content\n",
    "file_descriptor = open(file_name, encoding=\"utf8\")\n",
    "text = file_descriptor.read()\n",
    "\n",
    "print(text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69db3843",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f35461",
   "metadata": {},
   "source": [
    "A vocabulary is a sorted set of every character that appears in the text.\n",
    "\n",
    "It will then be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b150f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"'()*+,-./0123456789:;<>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz£﻿\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f5e4b3",
   "metadata": {},
   "source": [
    "### Encoding and decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cbc29b",
   "metadata": {},
   "source": [
    "With this vocabulary, we can now create a tokenizer, that consists of two parts : \n",
    "\n",
    "- encode text into integers sorted set, \n",
    "\n",
    "- and decode integers input into original text.\n",
    "\n",
    "This tokenizer works with char-level tokenizing, it means that in each prompt, it will encode each character. \n",
    "\n",
    "Each character will be affeted by an integer.\n",
    "\n",
    "It is not the most efficient, but we are gonna stay on char-level to simplify the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c1f9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: '*', 8: '+', 9: ',', 10: '-', 11: '.', 12: '/', 13: '0', 14: '1', 15: '2', 16: '3', 17: '4', 18: '5', 19: '6', 20: '7', 21: '8', 22: '9', 23: ':', 24: ';', 25: '<', 26: '>', 27: '?', 28: 'A', 29: 'B', 30: 'C', 31: 'D', 32: 'E', 33: 'F', 34: 'G', 35: 'H', 36: 'I', 37: 'J', 38: 'K', 39: 'L', 40: 'M', 41: 'N', 42: 'O', 43: 'P', 44: 'Q', 45: 'R', 46: 'S', 47: 'T', 48: 'U', 49: 'V', 50: 'W', 51: 'X', 52: 'Y', 53: 'Z', 54: '[', 55: ']', 56: 'a', 57: 'b', 58: 'c', 59: 'd', 60: 'e', 61: 'f', 62: 'g', 63: 'h', 64: 'i', 65: 'j', 66: 'k', 67: 'l', 68: 'm', 69: 'n', 70: 'o', 71: 'p', 72: 'q', 73: 'r', 74: 's', 75: 't', 76: 'u', 77: 'v', 78: 'w', 79: 'x', 80: 'y', 81: 'z', 82: '£', 83: '\\ufeff'}\n"
     ]
    }
   ],
   "source": [
    "# create a mmaping from characters to integers\n",
    "\n",
    "def string_to_int():\n",
    "   return { char:i for i, char in enumerate(vocab) }\n",
    "\n",
    "def int_to_string():\n",
    "   return { i:char for i, char in enumerate(vocab) }\n",
    "\n",
    "print(int_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e1a61",
   "metadata": {},
   "source": [
    "Then we can now use our converters to apply encoding and decoding :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db62dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a string, output a list of integers\n",
    "def encode(chars):\n",
    "   return [ string_to_int()[c] for c in chars ]\n",
    "\n",
    "# take a list of integers, output a string\n",
    "def decode(integers):\n",
    "   return ''.join([ int_to_string()[i] for i in integers ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7028702",
   "metadata": {},
   "source": [
    "Now let's encode the entire text dataset. Here is an use example of the tokenizer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a1590b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Encoded data: [83, 39, 70, 70, 66, 64, 69, 62, 1, 57, 56, 58, 66, 1, 75, 70, 1, 56, 67, 67, 1, 75, 63, 56, 75, 1, 63, 56, 74, 1, 70, 58, 58, 76, 73, 73, 60, 59, 1, 75, 70, 1, 68, 60, 1, 74, 64, 69, 58, 60, 1, 75, 63, 56, 75, 1, 60, 77, 60, 69, 75, 61, 76, 67, 1, 59, 56, 80, 9, 1, 36, 0, 56, 68, 1, 74, 58, 56, 73, 58, 60, 67, 80, 1, 56, 57, 67, 60, 1, 75, 70, 1, 57, 60, 67, 64, 60, 77, 60, 1] \n",
      "\n",
      "- Decoded data: ﻿Looking back to all that has occurred to me since that eventful day, I\n",
      "am scarcely able to believe \n"
     ]
    }
   ],
   "source": [
    "encoded_text = encode(text)\n",
    "\n",
    "print (\"- Encoded data:\", encoded_text[:100], \"\\n\")\n",
    "print (\"- Decoded data:\", decode(encoded_text[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f9605-a4cb-4aac-aa86-3adba7711a2d",
   "metadata": {},
   "source": [
    "We store it into a torch.Tensor :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7e61add-ef88-4811-9e76-a040d6b8f001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([484719]) torch.int64\n",
      "tensor([83, 39, 70, 70, 66, 64, 69, 62,  1, 57, 56, 58, 66,  1, 75, 70,  1, 56,\n",
      "        67, 67,  1, 75, 63, 56, 75,  1, 63, 56, 74,  1, 70, 58, 58, 76, 73, 73,\n",
      "        60, 59,  1, 75, 70,  1, 68, 60,  1, 74, 64, 69, 58, 60,  1, 75, 63, 56,\n",
      "        75,  1, 60, 77, 60, 69, 75, 61, 76, 67,  1, 59, 56, 80,  9,  1, 36,  0,\n",
      "        56, 68,  1, 74, 58, 56, 73, 58, 60, 67, 80,  1, 56, 57, 67, 60,  1, 75,\n",
      "        70,  1, 57, 60, 67, 64, 60, 77, 60,  1])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encoded_text, dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100]) # Print 100 first encoded character from the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffe3b2",
   "metadata": {},
   "source": [
    "## Predictions from contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d24eb",
   "metadata": {},
   "source": [
    "### Training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03dbf5",
   "metadata": {},
   "source": [
    "We splits data into prediction and validation, because we do not want to make the model to copy exactly the content of data, but try to product content similar to the data, not the exact one. \n",
    "\n",
    "So we never feed the entire data into the transform, but blocks of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d2ac67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 0.9 # Splits data\n",
    "n = int(len(data) * percent) # Calculated split\n",
    " \n",
    "train_data = data[:n+1] # first 90% data\n",
    "val_data = data[n:] # remaining 10% data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b6752",
   "metadata": {},
   "source": [
    "### Contexts and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddefaf2",
   "metadata": {},
   "source": [
    "The main goal of a model is from given context, achieve to predict what comes next. \n",
    "\n",
    "For example, if our text data is \"Hello\" :\n",
    "\n",
    "- If context: \"H\" -> target would predict: \"e\",\n",
    "\n",
    "- Then if context: \"e\" -> target would predict: \"l\",\n",
    "\n",
    "- Then if context: \"l\" -> target would predict: \"l\",\n",
    "\n",
    "- Then if context: \"l\" -> target would predict: \"o\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34560c8",
   "metadata": {},
   "source": [
    "### Usage of character blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d6e34",
   "metadata": {},
   "source": [
    "It would be better to work with blocks of characters, to gains more accuraty, and generate texts from given one. \n",
    "\n",
    "For example, if we keep our text data as \"Hello\" :\n",
    "\n",
    "- If context: \"H\" -> target would predict: \"e\",\n",
    "\n",
    "- Then if context: \"He\" -> target would predict: \"l\",\n",
    "\n",
    "- Then if context: \"Hel\" -> target would predict: \"l\",\n",
    "\n",
    "- Then if context: \"Hell\" -> target would predict: \"o\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a2fe1",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, a target always consists of the next predicted character, that comes after the context.\n",
    "\n",
    "So we can agree that :\n",
    "\n",
    "```python\n",
    "context_block = data[:block_size]\n",
    "\n",
    "target_block = data[1:block_size+1]\n",
    "```\n",
    "\n",
    "Here is an application, with a block size of 7 characters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a9884ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([83, 39, 70, 70, 66, 64, 69])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 7\n",
    "\n",
    "train_data[:block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca229a6b-72c7-4e16-9fb2-f4b80b0b601c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context block : tensor([83, 39, 70, 70, 66, 64, 69])\n",
      "Target block : tensor([39, 70, 70, 66, 64, 69, 62])\n"
     ]
    }
   ],
   "source": [
    "context_block = train_data[:block_size]\n",
    "target_block = train_data[1:block_size+1]\n",
    "\n",
    "print(\"Context block :\", context_block)\n",
    "print(\"Target block :\", target_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e4de8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "context: tensor([83]), target: 39\n",
      "---------------------------------------------------------------\n",
      "context: tensor([83, 39]), target: 70\n",
      "---------------------------------------------------------------\n",
      "context: tensor([83, 39, 70]), target: 70\n",
      "---------------------------------------------------------------\n",
      "context: tensor([83, 39, 70, 70]), target: 66\n",
      "---------------------------------------------------------------\n",
      "context: tensor([83, 39, 70, 70, 66]), target: 64\n",
      "---------------------------------------------------------------\n",
      "context: tensor([83, 39, 70, 70, 66, 64]), target: 69\n",
      "---------------------------------------------------------------\n",
      "context: tensor([83, 39, 70, 70, 66, 64, 69]), target: 62\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(block_size):\n",
    "    \n",
    "    context = context_block[:i+1]\n",
    "    target = target_block[i]\n",
    "\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(f\"context: {context}, target: {target}\")\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e012243",
   "metadata": {},
   "source": [
    "### Batchs for parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f64a90f",
   "metadata": {},
   "source": [
    "\n",
    "In a block, it would be more efficient to analyse predictions in parrallel within the block, because they are independant.\n",
    "\n",
    "So each step from previous loop, is a element of the batch.\n",
    "\n",
    "If batch_size = 4, a context batch would contains:\n",
    "\n",
    "- [[],[Lo],[Loo],[Loop]]\n",
    "\n",
    "We implement batch creation and processing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35136d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "# Generate a small batch of data of contexts x and targets y\n",
    "# Two possible splits : training or validation\n",
    "def get_batch (split) :\n",
    "\n",
    "    data = train_data if split == \"training\" else val_data\n",
    "    \n",
    "    # Create a tensor filled with random integers generated, of size block_size\n",
    "    ix = torch.randint(len(data) - block_size, (block_size,))\n",
    "    print(f\"- Random blocks adresses :\\n{ix}\\n\")\n",
    "\n",
    "    # Create context blocks from random coordinates\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "\n",
    "    # Create targets blocks from random coordinates\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    # batch of data of contexts x and targets y\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9df01227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Random blocks adresses :\n",
      "tensor([  5035, 192756, 148840, 106305, 225049, 102702,  66401, 389832])\n",
      "\n",
      "- contexts :\n",
      "tensor([[80,  1, 75, 70,  1, 70, 57, 74],\n",
      "        [63, 60,  1, 43, 73, 70, 61, 60],\n",
      "        [ 1, 75, 63, 60,  1, 60, 77, 60],\n",
      "        [67, 67, 80,  1, 63, 56, 59,  1],\n",
      "        [ 0,  3, 41, 70, 78,  9,  1, 75],\n",
      "        [75, 60, 73, 69, 56, 67, 67, 80],\n",
      "        [69, 70, 75, 63, 64, 69, 62,  1],\n",
      "        [65, 60, 58, 75, 74,  1, 64, 69]])\n",
      "\n",
      "- targets :\n",
      "tensor([[ 1, 75, 70,  1, 70, 57, 74, 60],\n",
      "        [60,  1, 43, 73, 70, 61, 60, 74],\n",
      "        [75, 63, 60,  1, 60, 77, 60, 73],\n",
      "        [67, 80,  1, 63, 56, 59,  1, 64],\n",
      "        [ 3, 41, 70, 78,  9,  1, 75, 63],\n",
      "        [60, 73, 69, 56, 67, 67, 80,  1],\n",
      "        [70, 75, 63, 64, 69, 62,  1, 58],\n",
      "        [60, 58, 75, 74,  1, 64, 69,  1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x,y = get_batch(\"training\");\n",
    "\n",
    "print(f\"- contexts :\\n{(x)}\\n\")\n",
    "print(f\"- targets :\\n{y}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0432a",
   "metadata": {},
   "source": [
    "To do parrallel computing, the best option is to do [GPU computing (or GPGPU)](https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units) with [CUDA API](https://en.wikipedia.org/wiki/CUDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b597f3",
   "metadata": {},
   "source": [
    "WORK IN PROGRESS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
